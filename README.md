---
# Building an AI-Powered Web Navigator for the Visually Impaired with .NET 8

---

Welcome to our workshop on enhancing web accessibility using .NET technologies and AI! Today, you'll learn how to build applications that make web navigation easier for visually impaired users. Get ready for coding sessions, exploring cutting-edge tools, and collaboration with fellow developers to create more accessible digital experiences.

---
## Workshop Objectives

- **Raise accessibility awareness**
- **Integrate Open AI with .NET**
- **Implement voice interaction**
- **Develop structured .NET 8 app**
- **Enhance web accessibility**

---
## Importance of Web Accessibility

- Over **2.2 billion people** have vision impairments globally.
- Many websites lack proper **accessibility features**.
- Accessibility enhances user experience for **everyone**.

---
## Challenges for Visually Impaired Users

- Navigating complex web layouts.
- Lack of descriptive text for images and buttons.
- Inaccessible forms and interactive elements.
- [Aké prekážky majú nevidiaci na webe? (youtube.com)](https://www.youtube.com/watch?v=Dwqu4NpT2go)

---

> [!cite]
> Thank you, Andrej Nemeček, for inspiring me to prepare this workshop.

---
## Why Web Accessibility Matters

- Empowers users with disabilities.
- Legal requirements in many countries.
- Enhances overall **user satisfaction**.

---
## Designing for Everyone

- Use semantic HTML elements.
- Provide alternative text for images.
- Ensure sufficient color contrast.

---
## Tools and Resources

- **Screen readers** for testing (e.g., NVDA, JAWS).
- Accessibility auditing tools (e.g., WAVE, Axe).
- Guidelines like **WCAG** (Web Content Accessibility Guidelines).

---
## Making a Difference

- Small changes can have a big impact.
- Inclusive design benefits all users.
- Let's commit to building **accessible applications**.

---
## Role of Technology in Accessibility

- **AI and .NET technologies** can enhance accessibility.
- Generative AI can **interpret and summarize** content.
- Voice assistants aid in **navigation and control**.

---
## Important Notice

- This workshop is generated by OpenAI's LLM.
- More specifically: **o1-preview** model.
- Iteratively adapted and reviewed. 

---
## .NET 

- Designed for **.NET developers**.
- We'll be using **C#** in the workshop.

---
## Windows OS Environment

- The workshop is prepared for the **Windows OS**.
- **Linux and macOS are not supported** for audio-related parts.
- Ensure you have a Windows machine for full participation.

---
## Prerequisites Checklist

- **Windows OS** machine (audio features not supported on Linux/macOS)
- Experience with **C# and .NET**
- **.NET 8 SDK** installed
- **Code editor/IDE** (e.g., Visual Studio Code)
- **Internet connectivity**
- **Headset with microphone** connected
- **OpenAI API Key** set as `OPENAI_API_KEY`

---
## Facing Issues?

1. **First**, use your **problem-solving skills**.
2. **Second**, ask your **fellow neighbor**.
3. **Third**, reach out to the **workshop facilitator**.
   - *Note: Assistance may be limited.*

---
## Workshop Structure

- Block 1: Introduction and Setup
- Block 2: Integrating OpenAI LLM
- Block 3: Integrating Voice Output
- Block 4: Integrating Voice Input
- Block 5: AI-Powered Commands

---
### Block 1: Introduction and Setup

- **Duration:** 50 min + 10 min break
- Understand the importance of accessible web navigation.
- Set up the development environment with **.NET 8**.

---
### Block 2: Integrating OpenAI LLM

- **Duration:** 50 min + 10 min break
- Use Playwright to navigate web pages.
- Summarize and interpret content using **OpenAI**.

---
### Block 3: Integrating Voice Output

- **Duration:** 50 min + 10 min break
- Integrate OpenAI's **Text-to-Speech API**.
- Enhance the app to read out AI-generated summaries.

---
### Block 4: Integrating Voice Input

- **Duration:** 50 min + 10 min break
- Integrate OpenAI's **Speech-to-Text API**.
- Navigate web pages based on voice input.

---

### Block 5: AI-Powered Commands

- **Duration:** 50 min + 10 min break
- Add AI-Powered Commands for interacting with web page.

---
## How to Code Along

- **Copy and paste** the provided code snippets.
- **Do not write code manually** by hand.
- This ensures **efficiency** and covers more areas in a short time.

---
## Final Reminders

---
### Stay Engaged

- Participate actively in discussions.
- Share your insights and experiences.
- Collaborate with your peers.

---
### Feedback is Welcome

- Your feedback helps us improve.
- Let us know what works and what doesn't.
- We're here to learn together.

---
### Contact Information

- **Workshop Facilitator:** Miľovčík, Peter
- peter.milovcik@siemens-healthineers.com
- https://www.linkedin.com/in/peter-milovcik/

---
## Ready to Begin?

- Let's dive into **Block 1** and start our journey!
- Prepare your systems and let's code!

---

https://bit.ly/4epYiSx

https://github.com/PeterMilovcik/AccessibleWebNavigator

---
# Workshop Block 1: Introduction, Setup, and Command Infrastructure with Dependency Injection

**Duration:** 50 min. + 10 min. break

Welcome to the workshop! In this first block, we'll set the foundation for our project by understanding the importance of accessible web navigation, setting up our development environment, and implementing a command infrastructure using the Command design pattern with Dependency Injection (DI). We'll organize our code using meaningful namespaces within a single project to maintain clarity and structure.

---

## **Objectives**

- **Understand the importance of accessible web navigation for the visually impaired.**
- **Learn about the role of AI and .NET technologies in enhancing accessibility.**
- **Set up the development environment with .NET 8.**
- **Create a base .NET 8 application with a well-structured code architecture using namespaces.**
- **Implement the Command design pattern to handle user commands asynchronously.**
- **Utilize Dependency Injection to manage dependencies and promote loose coupling.**
- **Implement the "exit" command as the first command in our application.**

---

## 1.1 Introduction to Accessible Web Navigation

### 1.1.1 Understanding the Challenges

- **Visual Impairment Statistics:**
  - Over **2.2 billion people** have a vision impairment globally.
  - Many websites are not designed with accessibility in mind.

- **Common Barriers:**
  - Difficulties in navigating complex web layouts.
  - Lack of descriptive text for images and buttons.
  - Inaccessible forms and interactive elements.
  - [Aké prekážky majú nevidiaci na webe? (youtube.com)](https://www.youtube.com/watch?v=Dwqu4NpT2go)

### 1.1.2 The Role of Technology

- **AI Solutions:**
  - Generative AI (LLM) can interpret and summarize content.
  - Voice assistants can aid in navigation and control.

- **Why .NET 8:**
  - Cross-platform capabilities (Windows, macOS, Linux).
  - Robust libraries and community support.
  - Supports modern code organization and architecture patterns.

---

## 1.2 Setting Up the Development Environment

### 1.2.1 Prerequisites

- **Hardware Requirements:**
  - A computer running Windows, macOS, or Linux.
  - At least 4GB of RAM.

- **Software Requirements:**
  - **.NET 8 SDK**.
  - Code editor or IDE (Visual Studio Code, Visual Studio, Rider, etc.).
  - Git (optional, for version control).

### 1.2.2 Installing .NET 8 SDK

#### **For Windows:**

1. **Download the Installer:**
   - Visit the [.NET Downloads page](https://dotnet.microsoft.com/download/dotnet/8.0).
   - Choose the installer for Windows.

2. **Run the Installer:**
   - Follow the on-screen instructions to complete the installation.

#### **For macOS:**

1. **Download the Installer:**
   - Go to the [.NET Downloads page](https://dotnet.microsoft.com/download/dotnet/8.0).
   - Select the installer for macOS.

2. **Install .NET 8 SDK:**
   - Open the downloaded `.pkg` file and follow the installation steps.

#### **For Linux:**

1. **Add Microsoft Package Repository:**
   - Open a terminal window.
   - Follow the instructions specific to your Linux distribution on the [.NET Linux Installation Guide](https://docs.microsoft.com/en-us/dotnet/core/install/linux).

2. **Install the SDK:**
   - Run the appropriate command for your distribution, for example:
```bash
sudo apt-get update
sudo apt-get install -y dotnet-sdk-8.0
```

### 1.2.3 Verifying the Installation

- Open a terminal or command prompt.
- Run:
```bash
dotnet --version
```
- You should see the version number of .NET 8 SDK installed.

### 1.2.4 Setting Up a Code Editor

- **Visual Studio Code (Recommended):**
  - Download and install from [Visual Studio Code website](https://code.visualstudio.com/).
  - Install the C# extension for syntax highlighting and IntelliSense.

- **Alternative IDEs:**
  - **Visual Studio 2022** (Windows/macOS): Full-featured IDE.
  - **JetBrains Rider** (Cross-platform): Advanced features for .NET development.

---

## 1.3 Creating the Base .NET 8 Application with Clean Code Structure

In this workshop, we'll work with a single .NET 8 Console Application project, organizing our code into meaningful namespaces and folders to maintain a clean and maintainable codebase.

### 1.3.1 Creating the Console Application Project

1. **Create a New Console Application**

   Open your terminal or command prompt and navigate to your projects directory.

```bash
mkdir AccessibleWebNavigator
cd AccessibleWebNavigator
dotnet new console -n AccessibleWebNavigator
```

2. **Open the Project in Your Code Editor**

```bash
code .
```

**Note**: If using Visual Studio Code.

### 1.3.2 Organizing Code with Namespaces and Folders

To maintain clarity and separation of concerns, we'll organize our code into different namespaces and folders within the single project.

#### Namespaces and Their Purpose

- **AccessibleWebNavigator.Commands**

  - **Purpose:** Contains command interfaces and implementations using the Command design pattern.
  - **Explanation:** This namespace encapsulates all the commands that the application can execute, making it easy to add, modify, or remove commands without affecting other parts of the application.

- **AccessibleWebNavigator.Services**

  - **Purpose:** Contains service interfaces and their implementations for various functionalities like API key management, web navigation, etc.
  - **Explanation:** Services provide reusable functionalities that can be injected wherever needed, promoting loose coupling and testability.

### 1.3.3 Creating Folders and Namespaces

In your project, create the following folders:

- **Commands**
- **Services**

Each folder corresponds to a namespace in your code.

---

## 1.4 Implementing the Command Design Pattern with Dependency Injection

We'll now set up the command infrastructure using the Command design pattern, incorporating Dependency Injection to manage dependencies and promote loose coupling.

### 1.4.1 Defining the ICommand Interface

Create a new file in the **Commands** folder:

**Commands/ICommand.cs**

```csharp
namespace AccessibleWebNavigator.Commands;

public interface ICommand
{
    bool CanExecute(string commandInput);
    Task<string> ExecuteAsync(string commandInput);
}
```

- **Explanation:** The `ICommand` interface includes:
  - A **non-async** method `CanExecute` that determines whether the command can handle the given `commandInput`.
  - An **async** method `ExecuteAsync` that performs the command's action and returns a `string` result.

### 1.4.2 Implementing the CommandInvoker to Use DI

Add the `CommandInvoker` to accept commands via Dependency Injection.

**Commands/CommandInvoker.cs**

```csharp
namespace AccessibleWebNavigator.Commands;

public class CommandInvoker
{
    private readonly IEnumerable<ICommand> _commands;

    public CommandInvoker(IEnumerable<ICommand> commands)
    {
        _commands = commands ?? throw new ArgumentNullException(nameof(commands));
    }

    public async Task<string> ExecuteCommandAsync(string commandInput)
    {
        foreach (var command in _commands)
        {
            if (command.CanExecute(commandInput))
            {
                return await command.ExecuteAsync(commandInput);
            }
        }

        return $"Unknown command: {commandInput}";
    }
}
```

- **Explanation:**
  - **Constructor Injection:** The `CommandInvoker` receives an `IEnumerable<ICommand>` via constructor injection.

### 1.4.3 Implementing the ExitCommand

Create a new file in the **Commands** folder:

**Commands/ExitCommand.cs**

```csharp
namespace AccessibleWebNavigator.Commands;

public class ExitCommand : ICommand
{
    public bool CanExecute(string commandInput) => 
        commandInput.Trim().StartsWith("exit", StringComparison.OrdinalIgnoreCase);

    public Task<string> ExecuteAsync(string commandInput) => 
        Task.FromResult("Exiting application.");
}
```

- **Explanation:**
  - **CanExecute:** Checks if the `commandInput` matches the command's trigger word ("exit").
  - **ExecuteAsync:** Returns a success message asynchronously.

### 1.4.4 Setting Up Dependency Injection

To manage dependencies and promote loose coupling, we'll set up a `ServiceProvider` using .NET's built-in Dependency Injection framework.

#### 1.4.4.1 Installing the Dependency Injection Package

Open a terminal in your project directory and run:

```bash
cd .\AccessibleWebNavigator\
dotnet add package Microsoft.Extensions.DependencyInjection
```

#### 1.4.4.2 Configuring Services

Create a new file in the **Services** folder:

**Services/ServiceConfigurator.cs**

```csharp
using Microsoft.Extensions.DependencyInjection;
using AccessibleWebNavigator.Commands;

namespace AccessibleWebNavigator.Services;

public static class ServiceConfigurator
{
    public static ServiceProvider Configure()
    {
        var services = new ServiceCollection();

        services.AddSingleton<ICommand, ExitCommand>();
        services.AddSingleton<CommandInvoker>();

        // Register other services as needed in the future

        return services.BuildServiceProvider();
    }
}
```

- **Explanation:**
  - **ServiceCollection:** Collects service descriptors.
  - **AddSingleton:** Registers services with a singleton lifetime.
  - **Configure:** Builds the `ServiceProvider` that will be used to resolve dependencies.

---

## 1.5 Updating the Program Entry Point

Modify the `Program.cs` file to utilize Dependency Injection and handle asynchronous execution.

**Program.cs**

```csharp
using Microsoft.Extensions.DependencyInjection;
using AccessibleWebNavigator.Commands;
using AccessibleWebNavigator.Services;

namespace AccessibleWebNavigator
{
    class Program
    {
        static async Task Main(string[] args)
        {
            var serviceProvider = ServiceConfigurator.Configure();

            var commandInvoker = serviceProvider.GetRequiredService<CommandInvoker>();

            Console.WriteLine("Welcome to Accessible Web Navigator!");
            Console.WriteLine("Type 'exit' to quit the application.");

            bool isRunning = true;

            while (isRunning)
            {
                Console.Write("Enter command: ");
                var commandInput = Console.ReadLine() ?? string.Empty;

                var result = await commandInvoker.ExecuteCommandAsync(commandInput);

                Console.WriteLine(result);

                if (commandInput.Trim().Equals("exit", StringComparison.OrdinalIgnoreCase))
                {
                    isRunning = false;
                }
            }
        }
    }
}
```

- **Explanation:**
  - **ServiceProvider:** Obtained by calling `ServiceConfigurator.Configure()`.
  - **Resolving Dependencies:** Uses `GetRequiredService<CommandInvoker>()` to get an instance of `CommandInvoker`.
  - **Async Execution:** The `Main` method is `async` to accommodate asynchronous command execution.

---

## 1.6 Recap and Next Steps

### 1.6.1 What We've Achieved

- **Set up a single .NET 8 Console Application project.**
- **Organized code into meaningful namespaces and folders.**
- **Implemented the Command design pattern to handle user commands asynchronously.**
- **Utilized Dependency Injection to manage dependencies and promote loose coupling.**
- **Created and registered the "exit" command using DI.**
- **Prepared the command infrastructure for future commands to be added in upcoming blocks.**

### 1.6.2 Understanding the Namespace Structure

By organizing our code into namespaces and folders, and using Dependency Injection, we've made our application more maintainable and scalable:

- **Commands Namespace:**
  - Houses all command-related interfaces and classes.
  - Commands are registered and managed via Dependency Injection.

- **Services Namespace:**
  - Contains the `ServiceConfigurator` class that sets up the DI container.
  - Will house other service-related classes in future blocks.

### 1.6.3 Preparing for Block 2

In the next block, we'll:

- **Integrate OpenAI's LLM to process commands that interact with web content.**
- **Implement commands for navigating to a web page and summarizing its content.**
- **Use the command infrastructure and DI to add new commands with `CanExecute` and asynchronous execution.**

---

## 1.7 Additional Resources

- **Dependency Injection in .NET:** [Microsoft Docs - Dependency Injection](https://learn.microsoft.com/en-us/dotnet/core/extensions/dependency-injection)
- **Command Design Pattern:** [Command Pattern - DoFactory](https://www.dofactory.com/net/command-design-pattern)
- **Asynchronous Programming in C#:** [Asynchronous Programming with async and await | Microsoft Docs](https://learn.microsoft.com/en-us/dotnet/csharp/asynchronous-programming/)
- **Namespaces in C#:** [Namespaces - C# Programming Guide | Microsoft Docs](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/namespaces/)

---

## 1.8 Q&A Session

Feel free to ask any questions or share any challenges you encountered during this block.

---

## Next Steps

In the upcoming blocks, we'll continue building on this foundation by integrating web navigation, AI-powered content summarization, and voice interaction capabilities, all while utilizing Dependency Injection to manage our application's dependencies effectively. Stay tuned!

---

Thank you for your participation so far! Let's take a short break before we move on to Block 2.

---

# Workshop Block 2: Integrating OpenAI LLM and Implementing Navigation and Summarization Commands

**Duration:** 50 min. + 10 min. break

Welcome back to the workshop! In this block, we'll enhance our application by integrating OpenAI's Language Model (LLM) and implementing commands for web navigation and content summarization. We'll continue to use the Command design pattern and Dependency Injection (DI) to maintain a clean and organized codebase.

---

## **Objectives**

- **Integrate OpenAI's LLM into the application.**
- **Implement the "navigate" command using the new `NavigateToAsync` method.**
- **Implement the "summarize" command to summarize web page content.**
- **Provide updated interfaces and implementations for `IApiKeyProvider` and `IWebNavigator`.**
- **Ensure commands handle user input and output messages appropriately.**
- **Maintain clean code architecture using namespaces and DI.**

---

## **Prerequisites**

Before we begin, ensure you have:

- Completed Block 1 and have the base application with command infrastructure set up.
- An active OpenAI API key stored in your environment variables (`OPENAI_API_KEY`).
- Internet connectivity for accessing web pages and OpenAI's API.

> [!help]
> If you don't have your own OpenAI API key or prefer not to use it for this workshop, please contact the workshop facilitator.

---

## 2.1 Integrating OpenAI's .NET SDK

We'll start by integrating OpenAI's .NET SDK into our project to interact with OpenAI's APIs.

### 2.1.1 Installing the OpenAI .NET SDK

Open a terminal in your project directory and run:

```bash
dotnet add package OpenAI --prerelease
```

- **Explanation:** This command adds the OpenAI .NET SDK to our project, allowing us to interact with OpenAI's services.

> [!tip]
> Using the prerelease version of the OpenAI NuGet package allows access to the latest features, early testing opportunities, experimental APIs, and ensures updates align with rapidly evolving AI advancements.

---

## 2.2 Setting Up the OpenAI Service

We'll create a service to handle interactions with OpenAI's API.

### 2.2.1 Creating the IApiKeyProvider Interface

Create a new file in the **Services** folder:

**Services/IApiKeyProvider.cs**

```csharp
namespace AccessibleWebNavigator.Services;

public interface IApiKeyProvider
{
    string GetOpenAiApiKey();
}
```

- **Explanation:** The `IApiKeyProvider` interface defines a method to retrieve the OpenAI API key.

### 2.2.2 Implementing the EnvironmentApiKeyProvider

Create a new file in the **Services** folder:

**Services/EnvironmentApiKeyProvider.cs**

```csharp
namespace AccessibleWebNavigator.Services;

public class EnvironmentApiKeyProvider : IApiKeyProvider
{
    public string GetOpenAiApiKey() => 
        Environment.GetEnvironmentVariable("OPENAI_API_KEY") ?? string.Empty;
}
```

- **Explanation:** The `EnvironmentApiKeyProvider` retrieves the OpenAI API key from the environment variable called `OPENAI_API_KEY`.

### 2.2.3 Creating the IOpenAIService Interface

Create a new file in the **Services** folder:

**Services/IOpenAIService.cs**

```csharp
namespace AccessibleWebNavigator.Services;

public interface IOpenAIService
{
    Task<string> SummarizeTextAsync(string textContent);
}
```

- **Explanation:** Defines a method to summarize text content asynchronously.

### 2.2.4 Implementing the OpenAIService

Create a new file in the **Services** folder:

**Services/OpenAIService.cs**

```csharp
using OpenAI.Chat;

namespace AccessibleWebNavigator.Services;

public class OpenAIService : IOpenAIService
{
    private readonly ChatClient _chatClient;

    public OpenAIService(IApiKeyProvider apiKeyProvider)
    {
        var apiKey = apiKeyProvider.GetOpenAiApiKey();
        _chatClient = new ChatClient("gpt-3.5-turbo", apiKey);
        // If you have your own API Key, you can try different models as well, but be aware of the billing costs
        // here are some suggestions: gpt-4, gpt-4-turbo, gpt-4o-mini, o1-mini, o1-preview
        // for more details about models and their parameters, visit: https://platform.openai.com/docs/models
        // if you want to learn more about model pricing, visit: https://openai.com/api/pricing/
    }

    public async Task<string> SummarizeTextAsync(string textContent)
    {
        var messages = new List<ChatMessage>
        {
            new SystemChatMessage("You are a helpful assistant that summarizes web page content."),
            new UserChatMessage($"Please provide a concise summary of the following content:\n\n{textContent}")
        };
        return await GenerateAsync(messages);
    }

    private async Task<string> GenerateAsync(List<ChatMessage> messages)
    {
        try
        {
            var response = await _chatClient.CompleteChatAsync(messages);
            return response.Value.ToString();
        }
        catch (Exception ex)
        {
            Console.Error.WriteLine($"An error occurred while generating the response: {ex.Message}");
            return $"An error occurred while generating the response. Please try again later.";
        }
    }
}
```

- **Explanation:**
	- **Constructor:** Initializes the OpenAI service with the provided API key.
	- **SummarizeTextAsync:** Sends a request to OpenAI's Chat Completion API to generate a summary.

---

## 2.3 Implementing the WebNavigator Service

We'll create a service to navigate to web pages and retrieve content, accepting a Playwright's shared `IPage` instance.

### 2.3.1 Installing Playwright

1. **Add Playwright to the project**

```bash
dotnet add package Microsoft.Playwright
dotnet build
```

2. **Install Playwright Browsers**
    
    After adding the package, install the necessary browsers:

```bash
cd ..
dotnet tool install --global Microsoft.Playwright.CLI
playwright install
```

**Note 1:** If you encounter permission issues, you might need to run the install command with `sudo` (on macOS/Linux) or as an administrator (on Windows).
**Note 2:** If you encounter playwright install issues, you might need to do the build command with `dotnet build`.

### 2.3.2 Creating the IWebNavigator Interface

Create a new file in the **Services** folder:

**Services/IWebNavigator.cs**

```csharp
namespace AccessibleWebNavigator.Services;

public interface IWebNavigator
{
    Task<bool> NavigateToAsync(string url);
    Task<string> GetInnerTextContentAsync();
}
```

**Explanation:**
- `NavigateToAsync` navigates to the given URL and returns a boolean indicating success or failure.
- `GetInnerTextContentAsync` retrieves the inner text content of the current page.

### 2.3.3 Implementing the WebNavigator

Create a new file in the **Services** folder:

**Services/WebNavigator.cs**

```csharp
using Microsoft.Playwright;
using System.Net;

namespace AccessibleWebNavigator.Services;

public class WebNavigator : IWebNavigator
{
    private readonly IPage _page;

    public WebNavigator(IPage page)
    {
        _page = page ?? throw new ArgumentNullException(nameof(page));
    }

    public async Task<bool> NavigateToAsync(string url)
    {
        try
        {
            var response = await _page.GotoAsync(url);
            return response == null || response.Status == (int)HttpStatusCode.OK;
        }
        catch (Exception ex)
        {
            Console.Error.WriteLine($"Error navigating to {url}: {ex.Message}");
            return false;
        }
    }

    public async Task<string> GetInnerTextContentAsync()
    {
        try
        {
            return await _page.InnerTextAsync("body");
        }
        catch (Exception ex)
        {
            Console.Error.WriteLine($"Error getting page content: {ex.Message}");
            return string.Empty;
        }
    }
}
```

**Explanation:**

- `NavigateToAsync` navigates to the specified URL and returns `true` if successful, `false` otherwise.
- `GetInnerTextContentAsync` retrieves the content of the current page.

## 2.4 Implementing the NavigateCommand

We'll create a command that navigates to a specified web page.

Create a new file in the **Commands** folder:

**Commands/NavigateCommand.cs**

```csharp
using AccessibleWebNavigator.Services;

namespace AccessibleWebNavigator.Commands;

public class NavigateCommand : ICommand
{
    private readonly IWebNavigator _webNavigator;

    public NavigateCommand(IWebNavigator webNavigator)
    {
        _webNavigator = webNavigator;
    }

    public bool CanExecute(string commandInput)
    {
        var lowerInput = commandInput.Trim().ToLower();
        return lowerInput.StartsWith("navigate to") || lowerInput.StartsWith("go to");
    }

    public async Task<string> ExecuteAsync(string commandInput)
    {
        var urlStartIndex = commandInput.IndexOf("to", StringComparison.OrdinalIgnoreCase) + 2;
        var url = commandInput.Substring(urlStartIndex).Trim();

        if (!url.StartsWith("http", StringComparison.OrdinalIgnoreCase))
        {
            url = "https://" + url;
        }

        Console.WriteLine($"Navigating to {url}...");

        var success = await _webNavigator.NavigateToAsync(url);

        return success ? $"Page '{url}' loaded successfully." : $"Failed to load page.";
    }
}
```

**Explanation:**

- `ExecuteAsync` uses `NavigateToAsync` to navigate to the URL.
- Returns a success or error message based on the result.

## 2.5 Implementing the SummarizeCommand

We'll create a command that summarizes a specified web page.

Create a new file in the **Commands** folder:

**Commands/SummarizeCommand.cs**

```csharp
using AccessibleWebNavigator.Services;

namespace AccessibleWebNavigator.Commands;

public class SummarizeCommand : ICommand
{
    private readonly IWebNavigator _webNavigator;
    private readonly IOpenAIService _openAIService;

    public SummarizeCommand(IWebNavigator webNavigator, IOpenAIService openAIService)
    {
        _webNavigator = webNavigator ?? throw new ArgumentNullException(nameof(webNavigator));
        _openAIService = openAIService ?? throw new ArgumentNullException(nameof(openAIService));
    }

    public bool CanExecute(string commandInput)
    {
        var lowerInput = commandInput.Trim().TrimEnd('.').ToLower();
        return 
            lowerInput == "summarize page" || 
            lowerInput == "summarize the page" || 
            lowerInput == "describe page" || 
            lowerInput == "describe the page";
    }

    public async Task<string> ExecuteAsync(string commandInput)
    {
        var content = await _webNavigator.GetInnerTextContentAsync();

        if (string.IsNullOrEmpty(content))
        {
            return "No page content available. Please navigate to a page first.";
        }

        var summary = await _openAIService.SummarizeTextAsync(content);

        return !string.IsNullOrEmpty(summary) ? summary : "Failed to generate summary.";
    }
}
```

**Explanation:**

- Retrieves page content directly from `IWebNavigator`.

## 2.6 Updating the Service Configuration

In this section, we'll update the `ServiceConfigurator` class to include:

- Registration of the `IOpenAIService`.
- Registration of the `IWebNavigator`.
- Initialization and registration of the shared `IPage` instance.
- Registration of the new commands.

### 2.6.1 Updating `ServiceConfigurator.cs`

**Updated `ServiceConfigurator.cs`:**

```csharp
using Microsoft.Extensions.DependencyInjection;
using AccessibleWebNavigator.Commands;
using Microsoft.Playwright;

namespace AccessibleWebNavigator.Services
{
    public static class ServiceConfigurator
    {
        public static async Task<ServiceProvider> ConfigureAsync()
        {
            var services = new ServiceCollection();

            // Register services
            services.AddSingleton<IApiKeyProvider, EnvironmentApiKeyProvider>();
            services.AddSingleton<IOpenAIService, OpenAIService>();

            // Initialize Playwright, Browser, and Page
            var playwright = await Playwright.CreateAsync();
            var browser = await playwright.Chromium.LaunchAsync(new BrowserTypeLaunchOptions { Headless = false });
            var page = await browser.NewPageAsync();

            // Register IPage as a singleton
            services.AddSingleton(page);

            // Register IWebNavigator, which depends on IPage
            services.AddSingleton<IWebNavigator, WebNavigator>();

            // Register commands
            services.AddSingleton<ICommand, ExitCommand>();
            services.AddSingleton<ICommand, NavigateCommand>();
            services.AddSingleton<ICommand, SummarizeCommand>();

            // Register the CommandInvoker
            services.AddSingleton<CommandInvoker>();

            return services.BuildServiceProvider();
        }
    }
}
```

**Explanation:**

- **Asynchronous Configuration Method:**
  - Changed `Configure` to `ConfigureAsync` to handle asynchronous initialization of Playwright components.
  - This allows us to await the creation of Playwright and browser instances.

- **Registering `IApiKeyProvider` and `IOpenAIService`:**
  - `IApiKeyProvider` is registered to provide the OpenAI API key from environment variables.
  - `IOpenAIService` is registered using a factory method to inject the API key retrieved from `IApiKeyProvider`.

- **Initializing and Registering `IPage`:**
  - We initialize Playwright and launch the browser with `Headless = false` to make the browser window visible.
  - We create a new page (`IPage` instance) and register it as a singleton in the DI container.
  - The shared `IPage` instance allows us to maintain browser state across multiple commands.

- **Registering `IWebNavigator`:**
  - `IWebNavigator` is registered and will receive the shared `IPage` instance via constructor injection.
  - This service provides methods `NavigateToAsync` and `GetPageContentAsync`.

- **Registering Commands:**
  - We register the commands `ExitCommand`, `NavigateCommand`, and `SummarizeCommand`.
  - Each command will receive its dependencies via constructor injection:
    - `NavigateCommand` depends on `IWebNavigator`.
    - `SummarizeCommand` depends on `IWebNavigator` and `IOpenAIService`.
    - `ExitCommand` does not have dependencies.

- **Registering `CommandInvoker`:**
  - `CommandInvoker` is registered to manage and execute commands.

### 2.6.2 Modifying `Program.cs` to Use `ServiceConfigurator`

Update `Program.cs` to use the asynchronous `ConfigureAsync` method from `ServiceConfigurator`.

**Updated `Program.cs`:**

```csharp
using Microsoft.Extensions.DependencyInjection;
using AccessibleWebNavigator.Commands;
using AccessibleWebNavigator.Services;

namespace AccessibleWebNavigator
{
    class Program
    {
        static async Task Main(string[] args)
        {
            var serviceProvider = await ServiceConfigurator.ConfigureAsync();

            var commandInvoker = serviceProvider.GetRequiredService<CommandInvoker>();

            Console.WriteLine("Welcome to Accessible Web Navigator!");
            Console.WriteLine("Type 'exit' to quit the application.");

            bool isRunning = true;

            while (isRunning)
            {
                Console.Write("Enter command: ");
                var commandInput = Console.ReadLine() ?? string.Empty;

                var result = await commandInvoker.ExecuteCommandAsync(commandInput);

                Console.WriteLine(result);

                if (commandInput.Trim().Equals("exit", StringComparison.OrdinalIgnoreCase))
                {
                    isRunning = false;
                }
            }
        }
    }
}
```

**Explanation:**

- **Awaiting `ConfigureAsync`:**
  - Since `ConfigureAsync` is now an asynchronous method, we await its completion to get the `ServiceProvider`.

- **No Changes to the Main Loop:**
  - The main loop remains unchanged, utilizing the `CommandInvoker` to process user commands.

## 2.7 Testing the Application 

1. **Run the Application**
    - Navigate to the App project directory and run:

```bash
cd .\AccessibleWebNavigator\
dotnet run
```

2. **Test Navigate Command**
    - When prompted to enter the command, enter for example:

```
navigate to https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment
```

3. **Test Summary Command**
	- When the page loaded successfully, enter the command:

```
summarize the page
```

4. **Review the Output**
    - The application should display the AI-generated summary.
    - Verify that the summary accurately reflects the content.

5. **Test Exit Command**
	- When the page content is summarized successfully, enter the command:

```
exit
```

- Application should exited successfully. 

## 2.8 Recap and Next Steps

### 2.8.1 What We've Achieved

In this block, we've significantly enhanced our application's capabilities:

- **Integrated OpenAI's LLM:**
  - Implemented `IOpenAIService` and `OpenAIService` to interact with OpenAI's API for text summarization.
  - Created `IApiKeyProvider` and `EnvironmentApiKeyProvider` to securely retrieve the OpenAI API key from environment variables.

- **Enhanced Web Navigation:**
  - Updated `IWebNavigator` with `NavigateToAsync` and `GetInnerTextContentAsync` methods.
  - Implemented `WebNavigator` to navigate to web pages and retrieve content using a shared `IPage` instance.

- **Implemented New Commands:**
  - **`NavigateCommand`:** Utilizes `NavigateToAsync` to navigate to specified URLs, providing success or error messages.
  - **`SummarizeCommand`:** Uses `GetInnerTextContentAsync` to retrieve page content and `IOpenAIService` to generate summaries.

- **Updated Service Configuration:**
  - Enhanced `ServiceConfigurator` to include all necessary services and commands.
  - Registered `IOpenAIService`, `IWebNavigator`, and the shared `IPage` instance.
  - Ensured all dependencies are correctly resolved via Dependency Injection.

- **Maintained Clean Code Architecture:**
  - Continued using meaningful namespaces and organized code for clarity.
  - Leveraged the Command design pattern to encapsulate command logic.
  - Promoted loose coupling and maintainability through Dependency Injection and interfaces.

- **Tested the Application:**
  - Verified that the `navigate` and `summarize` commands function as expected.
  - Ensured the browser window opens (`Headless = false`) and navigates to the specified URL.
  - Confirmed that summaries are generated and displayed correctly.

---

### 2.8.2 Q&A Session

Feel free to ask any questions or share any challenges you encountered during this block. I'm here to help!

---

### 2.8.3 Next Steps

In the next block, we'll focus on enhancing the accessibility of our application by adding voice interaction capabilities. To prepare for this, consider the following steps:

- **Prepare for Text-to-Speech Functionality:**
  - Integrate a Text-to-Speech (TTS) service to enable the application to read out summaries and other messages.
  - Explore cross-platform TTS libraries compatible with .NET.
  - Prepare your development environment for integrating TTS functionality.

- **Enhance Accessibility for Visually Impaired Users:**
  - Think about how the application's usability can be further improved.
  - Brainstorm additional features or commands that could enhance the user experience.

- **Review Codebase:**
  - Ensure you understand each component of the code written so far.
  - Identify areas that may need refactoring or additional comments for clarity.

By building upon the foundation laid in this block, we'll move closer to creating a fully accessible web navigation tool that leverages AI and modern .NET technologies to assist visually impaired users.

In Block 3, we'll take our application to the next level by integrating Text-to-Speech functionality, allowing users to receive auditory feedback and enhancing the overall accessibility of our tool.

---

### 2.8.4 Additional Resources

- **Playwright for .NET Documentation:** [Playwright for .NET](https://playwright.dev/dotnet/)
- **OpenAI .NET SDK:** [OpenAI .NET SDK GitHub](https://github.com/openai/openai-dotnet)
- **Dependency Injection in .NET:** [Microsoft Docs - Dependency Injection](https://learn.microsoft.com/en-us/dotnet/core/extensions/dependency-injection)

---

Thank you for your participation so far! Let's take a short break before we move on to Block 3.

---

# Workshop Block 3: Adding Voice Output with OpenAI's Text-to-Speech and Implementing a Question Answering Command

**Duration:** 50 min. + 10 min. break

Welcome back! In this block, we'll enhance our application by adding voice output capabilities using OpenAI's Text-to-Speech (TTS) API. This will enable the application to read out the AI-generated summaries and answers, making it more accessible for visually impaired users. We'll also implement a new command that allows users to ask questions about the web page content. We'll continue to focus on maintaining a clean and organized code structure by appropriately separating concerns into different layers.

---

## **Objectives**

- **Integrate OpenAI's Text-to-Speech (TTS) API** to convert text explanations into speech.
- **Enhance the application** to read out AI-generated summaries and answers.
- **Handle audio playback** for Windows operating system.
- **Implement a new command** to answer questions about the web page content.
- **Test the voice output** with various web pages.
- **Maintain clean code architecture** by organizing code into appropriate layers and using Dependency Injection.

---

## **Prerequisites**

- **Completion of Blocks 1 and 2**, with the application capable of navigating to web pages and summarizing content.
- **OpenAI API Key** stored in your environment variables (`OPENAI_API_KEY`).
- **Internet connectivity** for API access and web navigation.
- **Speakers or headphones** connected for audio output.

---

## 3.1 Understanding OpenAI's Text-to-Speech API

OpenAI's Text-to-Speech API allows you to convert text into spoken words using natural-sounding voices. We'll use this API to synthesize speech from the AI-generated summaries and answers.

For more details, visit: [https://platform.openai.com/docs/guides/text-to-speech/quickstart](https://platform.openai.com/docs/guides/text-to-speech/quickstart)

> [!warning]
> The maximum length for the input text is 4096 characters.

---

## 3.2 Setting Up the Environment

### 3.2.1 Ensure OpenAI API Key is Set

We already have the `OPENAI_API_KEY` environment variable set from previous blocks.

### 3.2.2 Install Required NuGet Packages

We'll use **NAudio** for audio playback on Windows.

In your project directory, install the necessary packages:

```bash
dotnet add package NAudio
dotnet add package System.Net.Http.Json
```

- **Explanation:**
  - **NAudio:** Provides audio playback functionality.
  - **System.Net.Http.Json:** Adds extension methods for `HttpClient` to handle JSON content.

---

## 3.3 Creating the Text-to-Speech Service

We'll create a new service in the **Services** namespace to handle text-to-speech functionality.

### 3.3.1 Defining the `ITextToSpeechService` Interface

Create a new file in the **Services** folder:

**Services/ITextToSpeechService.cs**

```csharp
namespace AccessibleWebNavigator.Services;

public interface ITextToSpeechService
{
    Task SpeakAsync(string text);
}
```

- **Explanation:** Defines an asynchronous method `SpeakAsync` that converts the provided text to speech.

### 3.3.2 Implementing the `TextToSpeechService`

Create a new file in the **Services** folder:

**Services/TextToSpeechService.cs**

```csharp
using System.Net.Http.Headers;
using System.Net.Http.Json;
using NAudio.Wave;

namespace AccessibleWebNavigator.Services;

public class TextToSpeechService : ITextToSpeechService
{
    private readonly string _apiKey;

    public TextToSpeechService(IApiKeyProvider apiKeyProvider)
    {
        _apiKey = apiKeyProvider.GetOpenAiApiKey();
    }

    public async Task SpeakAsync(string text)
    {
        Console.WriteLine($"Synthesizing text to speech...");
        var audioData = await SynthesizeSpeechAsync(text);

        if (audioData != null && audioData.Length > 0)
        {
            Console.WriteLine("Playing audio...");
            await PlayAudioAsync(audioData);
            Console.WriteLine("Audio playback complete.");
        }
        else
        {
            Console.Error.WriteLine("Failed to synthesize speech.");
        }
    }

    private async Task<byte[]> SynthesizeSpeechAsync(string text)
    {
        try
        {
            using var client = new HttpClient();
            client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue("Bearer", _apiKey);

            var requestContent = new
            {
                model = "tts-1",
                input = text,
                voice = "onyx" // You can try different voices here
            };
            // for more voice options, visit: https://platform.openai.com/docs/guides/text-to-speech/quickstart

            var response = await client.PostAsJsonAsync("https://api.openai.com/v1/audio/speech", requestContent);

            if (response.IsSuccessStatusCode)
            {
                return await response.Content.ReadAsByteArrayAsync();
            }
            else
            {
                var error = await response.Content.ReadAsStringAsync();
                Console.WriteLine($"Error in TTS API call: {error}");
                return Array.Empty<byte>();
            }
        }
        catch (Exception ex)
        {
            Console.WriteLine($"An error occurred while synthesizing speech: {ex.Message}");
            return Array.Empty<byte>();
        }
    }

    private async Task PlayAudioAsync(byte[] audioData)
    {
        using var ms = new MemoryStream(audioData);
        using var rdr = new Mp3FileReader(ms);
        using var waveOut = new WaveOutEvent();
        waveOut.Init(rdr);
        waveOut.Play();
        while (waveOut.PlaybackState == PlaybackState.Playing)
        {
            await Task.Delay(500);
        }
    }
}
```

- **Explanation:**
  - **SynthesizeSpeechAsync:** Calls the OpenAI TTS API to synthesize speech from text.
  - **PlayAudioAsync:** Plays the synthesized audio using NAudio.
  - **SpeakAsync**: Combines text-to-speech synthesis and audio playback. 

**Note:** Ensure that the `NAudio` and `System.Net.Http.Json` packages are installed.

---

## 3.4 Updating the Service Configuration

### 3.4.1 Updating `ServiceConfigurator.cs`

Update the `ServiceConfigurator` to include the `ITextToSpeechService`.

**Updated `ServiceConfigurator.cs`:**

```csharp
using Microsoft.Extensions.DependencyInjection;
using AccessibleWebNavigator.Commands;
using Microsoft.Playwright;

namespace AccessibleWebNavigator.Services
{
    public static class ServiceConfigurator
    {
        public static async Task<ServiceProvider> ConfigureAsync()
        {
            var services = new ServiceCollection();

            // Register services
            services.AddSingleton<IApiKeyProvider, EnvironmentApiKeyProvider>();
            services.AddSingleton<IOpenAIService, OpenAIService>();
            services.AddSingleton<ITextToSpeechService, TextToSpeechService>();

            // Initialize Playwright, Browser, and Page
            var playwright = await Playwright.CreateAsync();
            var browser = await playwright.Chromium.LaunchAsync(new BrowserTypeLaunchOptions { Headless = false });
            var page = await browser.NewPageAsync();

            // Register IPage as a singleton
            services.AddSingleton(page);

            // Register IWebNavigator, which depends on IPage
            services.AddSingleton<IWebNavigator, WebNavigator>();

            // Register commands
            services.AddSingleton<ICommand, ExitCommand>();
            services.AddSingleton<ICommand, NavigateCommand>();
            services.AddSingleton<ICommand, SummarizeCommand>();

            // Register the CommandInvoker
            services.AddSingleton<CommandInvoker>();

            return services.BuildServiceProvider();
        }
    }
}
```

- **Explanation:**
  - **Registered `ITextToSpeechService`:** Adds the TTS service to the DI container, providing the API key.

---

## 3.5 Adapting `Program.cs` to Use Text-to-Speech

Modify `Program.cs` to utilize the `ITextToSpeechService` to make command outputs audible.

**Updated `Program.cs`:**

```csharp
using AccessibleWebNavigator.Commands;
using AccessibleWebNavigator.Services;
using Microsoft.Extensions.DependencyInjection;

namespace AccessibleWebNavigator
{
    class Program
    {
        static async Task Main(string[] args)
        {
            // Set up Dependency Injection
            var serviceProvider = await ServiceConfigurator.ConfigureAsync();

            var commandInvoker = serviceProvider.GetRequiredService<CommandInvoker>();
            var textToSpeechService = serviceProvider.GetRequiredService<ITextToSpeechService>();

            Console.WriteLine("Welcome to Accessible Web Navigator!");
            Console.WriteLine("Type 'exit' to quit the application.");

            bool isRunning = true;

            while (isRunning)
            {
                Console.Write("Enter command: ");
                var commandInput = Console.ReadLine() ?? string.Empty;

                var result = await commandInvoker.ExecuteCommandAsync(commandInput);

                Console.WriteLine(result);
                await textToSpeechService.SpeakAsync(result);

                if (commandInput.Trim().Equals("exit", StringComparison.OrdinalIgnoreCase))
                {
                    isRunning = false;
                }
            }
        }
    }
}
```

- **Explanation:**
  - **Retrieve `ITextToSpeechService`:** Gets the TTS service from the DI container.
  - **Asynchronous `SpeakAsync`:** Uses `await` to asynchronously speak the command output.
  - **Enhanced Accessibility:** All command outputs are now read aloud.

---

## 3.6 Implementing the Question Answering Command

We'll create a new command that allows users to ask questions about the content of the current web page.

### 3.6.1 Updating the `IOpenAIService` Interface

Add a new method to handle question answering.

**Updated `IOpenAIService.cs`:**

```csharp
namespace AccessibleWebNavigator.Services;

public interface IOpenAIService
{
    Task<string> SummarizeTextAsync(string textContent);
    Task<string> AnswerQuestionAsync(string textContent, string question);
}
```

### 3.6.2 Implementing the `AnswerQuestionAsync` Method

Update the `OpenAIService` class.

**Updated `OpenAIService.cs`:**

```csharp
using OpenAI.Chat;

namespace AccessibleWebNavigator.Services;

public class OpenAIService : IOpenAIService
{
    private readonly ChatClient _chatClient;

    public OpenAIService(IApiKeyProvider apiKeyProvider)
    {
        var apiKey = apiKeyProvider.GetOpenAiApiKey();
        _chatClient = new ChatClient("gpt-3.5-turbo", apiKey);
        // If you have your own API Key, you can try different models as well, but be aware of the billing costs
        // here are some suggestions: gpt-4, gpt-4-turbo, gpt-4o-mini, o1-mini, o1-preview
        // for more details about models and their parameters, visit: https://platform.openai.com/docs/models
        // if you want to learn more about model pricing, visit: https://openai.com/api/pricing/
    }

    public async Task<string> SummarizeTextAsync(string textContent)
    {
        var messages = new List<ChatMessage>
        {
            new SystemChatMessage("You are a helpful assistant that summarizes web page content."),
            new UserChatMessage($"Please provide a concise summary of the following content:\n\n{textContent}")
        };
        return await GenerateAsync(messages);
    }

    public async Task<string> AnswerQuestionAsync(string textContent, string question)
    {
        var messages = new List<ChatMessage>
        {
            new SystemChatMessage("You are a helpful assistant that answers questions based on the provided web page content."),
            new UserChatMessage(
                $"Based on the following web page content, please answer the question:\n\n" + 
                $"Content:\n{textContent}\n\nQuestion:\n{question}")
        };

        return await GenerateAsync(messages);
    }

    private async Task<string> GenerateAsync(List<ChatMessage> messages)
    {
        try
        {
            var response = await _chatClient.CompleteChatAsync(messages);
            return response.Value.ToString();
        }
        catch (Exception ex)
        {
            Console.Error.WriteLine($"An error occurred while generating the response: {ex.Message}");
            return $"An error occurred while generating the response. Please try again later.";
        }
    }
}
```

- **Explanation:**
  - **AnswerQuestionAsync:** Sends a prompt to OpenAI's API to get an answer based on the provided content and question.

### 3.6.3 Creating the `QuestionAnswerCommand`

Create a new file in the **Commands** folder:

**Commands/QuestionAnswerCommand.cs**

```csharp
using AccessibleWebNavigator.Services;

namespace AccessibleWebNavigator.Commands;

public class QuestionAnswerCommand : ICommand
{
    private const string CommandWord = "question ";
    private readonly IWebNavigator _webNavigator;
    private readonly IOpenAIService _openAIService;

    public QuestionAnswerCommand(IWebNavigator webNavigator, IOpenAIService openAIService)
    {
        _webNavigator = webNavigator ?? throw new ArgumentNullException(nameof(webNavigator));
        _openAIService = openAIService ?? throw new ArgumentNullException(nameof(openAIService));
    }

    public bool CanExecute(string commandInput) => 
        commandInput.Trim().ToLower().StartsWith(CommandWord);

    public async Task<string> ExecuteAsync(string commandInput)
    {
        var question = commandInput.Substring(CommandWord.Length).Trim();

        if (string.IsNullOrEmpty(question))
        {
            return $"Please provide a question after a command word '{CommandWord}'.";
        }

        var content = await _webNavigator.GetInnerTextContentAsync();

        if (string.IsNullOrEmpty(content))
        {
            return "No page content available. Please navigate to a page first.";
        }

        var answer = await _openAIService.AnswerQuestionAsync(content, question);

        return answer;
    }
}
```

- **Explanation:**
  - **CanExecute:** Checks if the command starts with "question ".
  - **ExecuteAsync:** Retrieves the page content and uses the AI service to answer the user's question based on the content.

### 3.6.4 Updating the Service Configuration

Now that we've implemented the `QuestionAnswerCommand`, we need to register it with our Dependency Injection container so that it can be recognized and used by the `CommandInvoker`. We'll update the `ServiceConfigurator` class to include this new command.

**Updated `ServiceConfigurator.cs`:**

```csharp
using Microsoft.Extensions.DependencyInjection;
using AccessibleWebNavigator.Commands;
using Microsoft.Playwright;

namespace AccessibleWebNavigator.Services;

public static class ServiceConfigurator
{
    public static async Task<ServiceProvider> ConfigureAsync()
    {
        var services = new ServiceCollection();

        // Register services
        services.AddSingleton<IApiKeyProvider, EnvironmentApiKeyProvider>();
        services.AddSingleton<IOpenAIService, OpenAIService>();
        services.AddSingleton<ITextToSpeechService, TextToSpeechService>();

        // Initialize Playwright, Browser, and Page
        var playwright = await Playwright.CreateAsync();
        var browser = await playwright.Chromium.LaunchAsync(new BrowserTypeLaunchOptions { Headless = false });
        var page = await browser.NewPageAsync();

        // Register IPage as a singleton
        services.AddSingleton(page);

        // Register IWebNavigator, which depends on IPage
        services.AddSingleton<IWebNavigator, WebNavigator>();

        // Register commands
        services.AddSingleton<ICommand, ExitCommand>();
        services.AddSingleton<ICommand, NavigateCommand>();
        services.AddSingleton<ICommand, SummarizeCommand>();
        services.AddSingleton<ICommand, QuestionAnswerCommand>();

        // Register the CommandInvoker
        services.AddSingleton<CommandInvoker>();

        return services.BuildServiceProvider();
    }
}
```

**Explanation:**

- **Registering `QuestionAnswerCommand`:** We've added the line:

```csharp
services.AddSingleton<ICommand, QuestionAnswerCommand>();
```

This registers the `QuestionAnswerCommand` with the Dependency Injection container as a singleton implementation of `ICommand`. Now, when the `CommandInvoker` searches for commands to execute, it will include `QuestionAnswerCommand` in its list of available commands.

---

## 3.7 Testing the Application

### 3.7.1 Running the Application

```bash
dotnet run
```

### 3.7.2 Testing the Text-to-Speech Functionality

1. **Navigate to a Web Page:**

   - **Input:**

```
navigate to https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment
```
   
   - **Expected Behavior:**
     - The browser window opens and navigates to `https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment`.
     - You hear "Page loaded successfully."

2. **Summarize the Page:**

   - **Input:** 

```
summarize the page
```

   - **Expected Behavior:**
     - The summary is displayed in the console.
     - The summary is read aloud using TTS.

3. **Ask a Question:**

   - **Input:** 

```
question What are leading causes of vision impairment and blindness?
```

   - **Expected Behavior:**
     - The assistant provides an answer based on the page content.
     - The answer is displayed in the console and read aloud.

4. **Exit the Application:**

   - **Input:** `exit`
   - **Expected Behavior:**
     - "Exiting application."

### 3.7.3 Testing with Different Web Pages

- **Example URLs:**
  - `https://www.w3.org/WAI/tips/developing/`
  - `https://unss.sk/pristupnost-webovych-aplikacii/` (Slovak language)

- **Verify:**
  - The application can handle content in different languages.
  - The TTS service reads out the summaries and answers correctly.

---

## 3.8 Error Handling and Logging

### 3.8.1 Handling API Errors in `OpenAIService`

Ensure that any failures in API calls are properly handled and informative messages are provided to the user.

---

## 3.9 Additional Enhancements

### 3.9.1 Customizing Voice Attributes

OpenAI's TTS API supports different voices. Update the `requestContent` in `SynthesizeSpeechAsync` as needed.

```csharp
var requestContent = new
{
    model = "tts-1",
    input = text,
    voice = "onyx", // Try different voices here
};
```

> [!hint]
> For more voice options, visit the OpenAI TTS API documentation: https://platform.openai.com/docs/guides/text-to-speech/quickstart

### 3.9.2 Splitting Long Text into Chunks (Optional)

If the text is too long, split it into manageable chunks to comply with API limits.

```csharp
private async Task SpeakAsync(string text)
{
    const int maxChunkSize = 1000; // Adjust based on API limitations

    var chunks = SplitTextIntoChunks(text, maxChunkSize);

    foreach (var chunk in chunks)
    {
        var audioData = await SynthesizeSpeechAsync(chunk);

        if (audioData != null && audioData.Length > 0)
        {
            await PlayAudioAsync(audioData);
        }
        else
        {
            Console.WriteLine("Failed to synthesize speech for a chunk.");
        }
    }
}

private IEnumerable<string> SplitTextIntoChunks(string text, int maxChunkSize)
{
    for (int i = 0; i < text.Length; i += maxChunkSize)
    {
        yield return text.Substring(i, Math.Min(maxChunkSize, text.Length - i));
    }
}
```

- **Explanation:** Splits long texts into smaller chunks to avoid exceeding API limitations.

---

## 3.10 Recap and Next Steps

### 3.10.1 What We've Accomplished

- **Created a Text-to-Speech Service:**
  - Implemented `ITextToSpeechService` and `TextToSpeechService`.
  - Integrated OpenAI's TTS API to convert text into speech.
  - Handled audio playback using NAudio.

- **Enhanced the Application:**
  - Updated `Program.cs` to read out AI-generated summaries and answers.
  - Improved accessibility for visually impaired users.

- **Implemented the `QuestionAnswerCommand`:**
  - Allowed users to ask questions about the web page content.
  - Integrated the AI service and page's text content to provide answers.

- **Maintained Clean Code Architecture:**
  - Organized code into appropriate namespaces and layers.
  - Used Dependency Injection to manage services.

### 3.10.2 Looking Ahead to Block 4

In the next block, we'll:

- **Integrate Voice Commands:**
  - Use speech recognition to capture user commands.
  - Allow users to interact with the application using voice input.

- **Enhance Web Interaction:**
  - Utilize Playwright to perform actions based on voice input.
  - Focus on effectively identifying HTML elements based on user commands.

---

## 3.11 Troubleshooting Tips

- **No Sound Output:**
  - Verify that your audio devices are working and properly configured.
  - Check system volume settings.

- **Authentication Errors:**
  - Ensure that your OpenAI API key is correctly set in environment variables.

- **API Errors:**
  - Check for detailed error messages in the console output.
  - Ensure that the text being synthesized is not empty or exceeds API limits.

---

## 3.12 Additional Resources

- **OpenAI TTS API Documentation:** [Text-to-Speech Overview](https://platform.openai.com/docs/guides/text-to-speech/quickstart)
- **NAudio Documentation:** [NAudio GitHub Repository](https://github.com/naudio/NAudio)
- **Dependency Injection in .NET:** [Microsoft Docs - Dependency Injection](https://learn.microsoft.com/en-us/dotnet/core/extensions/dependency-injection)

---

## 3.13 Q&A Session

Feel free to ask any questions or share any difficulties you encountered during this block.

---

## 3.14 Summary

In this block, we've significantly improved our application by adding voice output capabilities using OpenAI's Text-to-Speech API and implementing a new command for question answering. By organizing our code into services and maintaining a clean architecture, we've made our application more maintainable and scalable.

---

## 3.15 Next Steps

In Block 4, we'll empower users to interact with web pages using voice commands, with a focus on effectively identifying HTML elements based on user input, bringing us closer to a fully interactive, accessible web navigation tool.

Thank you for your participation so far! Let's take a short break before we move on to Block 4.

---

# Workshop Block 4: Adding Speech-to-Text Capabilities

**Duration:** 50 min. + 10 min. break

Welcome back! In this block, we'll enhance our application by adding speech-to-text capabilities. This will allow users to issue commands via voice input, further improving the accessibility and user experience of our application. We'll utilize OpenAI's Whisper API for transcribing audio input into text commands.

---

## **Objectives**

- **Implement speech-to-text functionality using OpenAI's Whisper API.**
- **Create the `ISpeechToTextService` interface and its implementation.**
- **Implement the voice recording logic to use cancellation tokens and keyboard interaction.**
- **Update the main program loop to use voice input in combination with cancellation tokens.**
- **Maintain clean code architecture by organizing code into appropriate namespaces and using Dependency Injection.**

---

## **Prerequisites**

Before we begin, ensure you have:

- **Completed Blocks 1 to 3**, with the application capable of navigating to web pages, summarizing content, reading aloud, and answering questions.
- **An active OpenAI API key** stored in your environment variables (`OPENAI_API_KEY`).
- **Internet connectivity** for accessing OpenAI's API.
- **A microphone** connected to your computer for voice input.
- **Speakers or headphones** connected for audio output (optional but recommended).

---

## 4.1 Implementing Speech-to-Text Functionality

We'll capture voice input from the user using cross-platform audio input methods and transcribe it into text commands using OpenAI's Whisper API.

### 4.1.1 Creating the `ISpeechToTextService` Interface

Create a new interface in the **Services** folder:

**Services/ISpeechToTextService.cs**

```csharp
namespace AccessibleWebNavigator.Services;

public interface ISpeechToTextService
{
    Task<string> CaptureVoiceCommandAsync(CancellationToken cancellationToken);
}
```

- **Explanation:** Defines an asynchronous method `CaptureVoiceCommandAsync` that captures voice input and returns the transcribed text. It accepts a `CancellationToken` to handle cancellation of the recording process.

### 4.1.2 Implementing the `SpeechToTextService`

Create a new class in the **Services** folder:

**Services/SpeechToTextService.cs**

```csharp
using System.Net.Http.Headers;
using System.Text.Json;
using NAudio.Wave;

namespace AccessibleWebNavigator.Services;

public class SpeechToTextService : ISpeechToTextService
{
    private readonly string _apiKey;

    public SpeechToTextService(IApiKeyProvider apiKeyProvider)
    {
        if (apiKeyProvider == null)
        {
            throw new ArgumentNullException(nameof(apiKeyProvider));
        }
        _apiKey = apiKeyProvider.GetOpenAiApiKey();
    }

    public async Task<string> CaptureVoiceCommandAsync(CancellationToken cancellationToken)
    {
        string audioFilePath = Path.Combine(Path.GetTempPath(), "command.wav");
        try
        {
            await RecordAudioAsync(audioFilePath, cancellationToken);
			return await TranscribeAudioAsync(audioFilePath);
        }
        finally
        {
            RemoveTemporaryAudioFile(audioFilePath);
        }
    }

    private async Task RecordAudioAsync(string outputFilePath, CancellationToken cancellationToken)
    {
        using var waveIn = new WaveInEvent();
        await using var writer = new WaveFileWriter(outputFilePath, waveIn.WaveFormat);

        var tcs = new TaskCompletionSource<bool>();

        waveIn.DataAvailable += (sender, args) =>
        {
            writer.Write(args.Buffer, 0, args.BytesRecorded);
        };

        waveIn.RecordingStopped += (sender, args) =>
        {
            writer.Dispose();
            waveIn.Dispose();
            tcs.TrySetResult(true);
        };

        waveIn.WaveFormat = new WaveFormat(16000, 1); // 16 kHz, mono
        waveIn.StartRecording();

        await using (cancellationToken.Register(waveIn.StopRecording))
        {
            await tcs.Task;
        }
    }

    private async Task<string> TranscribeAudioAsync(string audioFilePath)
    {
        Console.WriteLine("Transcribing audio...");

        using var httpClient = new HttpClient();
        httpClient.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue("Bearer", _apiKey);

        var form = new MultipartFormDataContent
        {
            { new StringContent("whisper-1"), "model" },
            { new StringContent("en"), "language" },
            { new StringContent("json"), "response_format" },
            { new StringContent("Please transcribe voice command for web navigation to assist visually impaired users."), "prompt" }
        };

        var fileBytes = File.ReadAllBytes(audioFilePath);
        var fileContent = new ByteArrayContent(fileBytes)
        {
            Headers = { ContentType = new MediaTypeHeaderValue("audio/wav") }
        };
        form.Add(fileContent, "file", Path.GetFileName(audioFilePath));

        var response = await httpClient.PostAsync("https://api.openai.com/v1/audio/transcriptions", form);
        var responseContent = await response.Content.ReadAsStringAsync();

        if (response.IsSuccessStatusCode)
        {
            try
            {
                using var json = JsonDocument.Parse(responseContent);
                if (json.RootElement.TryGetProperty("text", out var textElement))
                {
                    return textElement.GetString() ?? "No transcription text found.";
                }
                else
                {
                    Console.WriteLine("Transcription text not found in response.");
                    return "Transcription text not found in response.";
                }
            }
            catch (JsonException ex)
            {
                Console.Error.WriteLine($"JSON parsing error: {ex.Message}");
                return "Error parsing transcription response.";
            }
        }
        else
        {
            Console.WriteLine($"Error in transcription: {responseContent}");
            return $"Error in transcription: {responseContent}";
        }
    }

    private static void RemoveTemporaryAudioFile(string audioFilePath)
    {
        if (File.Exists(audioFilePath))
        {
            File.Delete(audioFilePath);
        }
    }
}
```


---

### 4.1.3 Creating a Dedicated Class for Handling Voice Input

Create a new class to handle the keyboard start/stop interaction and manage the cancellation token.

**Services/VoiceInputHandler.cs**

```csharp
namespace AccessibleWebNavigator.Services;

public class VoiceInputHandler
{
    private readonly ISpeechToTextService _speechToTextService;

    public VoiceInputHandler(ISpeechToTextService speechToTextService)
    {
        _speechToTextService = speechToTextService ?? throw new ArgumentNullException(nameof(speechToTextService));
    }

    public async Task<string> GetVoiceCommandAsync()
    {
        using var cts = new CancellationTokenSource();

        Console.WriteLine("Press Enter to start recording your command.");
        Console.ReadLine();

        Console.WriteLine("Recording... Press Enter to stop.");
        var recordingTask = _speechToTextService.CaptureVoiceCommandAsync(cts.Token);

        // Wait for the user to press Enter to stop recording
        await Task.Run(() => Console.ReadLine());

        // Cancel the recording
        cts.Cancel();

        var commandText = await recordingTask;

        return commandText;
    }
}
```

- **Explanation:**
  - **VoiceInputHandler:** Manages the user interaction for starting and stopping the recording.
  - **CancellationTokenSource:** Used to signal cancellation to the `SpeechToTextService`.
  - **Keyboard Interaction:** Handled in this class, separating concerns from the `SpeechToTextService`.

---

## 4.2 Updating the Service Configuration

We need to register the `ISpeechToTextService` and `VoiceInputHandler` in our Dependency Injection container.

### 4.2.1 Updating `ServiceConfigurator.cs`

**Updated `ServiceConfigurator.cs`:**

```csharp
using Microsoft.Extensions.DependencyInjection;
using AccessibleWebNavigator.Commands;
using Microsoft.Playwright;

namespace AccessibleWebNavigator.Services
{
    public static class ServiceConfigurator
    {
        public static async Task<ServiceProvider> ConfigureAsync()
        {
            var services = new ServiceCollection();

            // Register services
            services.AddSingleton<IApiKeyProvider, EnvironmentApiKeyProvider>();
            services.AddSingleton<IOpenAIService, OpenAIService>();
            services.AddSingleton<ITextToSpeechService, TextToSpeechService>();
            services.AddSingleton<ISpeechToTextService, SpeechToTextService>();

            // Initialize Playwright, Browser, and Page
            var playwright = await Playwright.CreateAsync();
            var browser = await playwright.Chromium.LaunchAsync(new BrowserTypeLaunchOptions { Headless = false });
            var page = await browser.NewPageAsync();

            // Register IPage as a singleton
            services.AddSingleton(page);

            // Register IWebNavigator, which depends on IPage
            services.AddSingleton<IWebNavigator, WebNavigator>();

            // Register commands
            services.AddSingleton<ICommand, ExitCommand>();
            services.AddSingleton<ICommand, NavigateCommand>();
            services.AddSingleton<ICommand, SummarizeCommand>();
            services.AddSingleton<ICommand, QuestionAnswerCommand>();

            // Register utilities
            services.AddSingleton<CommandInvoker>();
            services.AddSingleton<VoiceInputHandler>();

            return services.BuildServiceProvider();
        }
    }
}
```

- **Explanation:**
  - **Registered `ISpeechToTextService`:** Adds the speech-to-text service to the DI container.
  - **Registered `VoiceInputHandler`:** So it can be injected where needed.

---

## 4.3 Updating the Main Program Loop

We will now update the `Program.cs` file to incorporate voice input, using the `SpeechToTextService` and `VoiceInputHandler`.

### 4.3.1 Modifying `Program.cs`

**Updated `Program.cs`:**

```csharp
using Microsoft.Extensions.DependencyInjection;
using AccessibleWebNavigator.Commands;
using AccessibleWebNavigator.Services;

namespace AccessibleWebNavigator
{
    class Program
    {
        static async Task Main(string[] args)
        {
            var serviceProvider = await ServiceConfigurator.ConfigureAsync();

            var commandInvoker = serviceProvider.GetRequiredService<CommandInvoker>();
            var textToSpeechService = serviceProvider.GetRequiredService<ITextToSpeechService>();
            var voiceInputHandler = serviceProvider.GetRequiredService<VoiceInputHandler>();

            Console.WriteLine("Welcome to Accessible Web Navigator!");
            Console.WriteLine("Type 'exit' to quit the application.");
            Console.WriteLine("You can also use voice commands.");

            bool isRunning = true;

            while (isRunning)
            {
                Console.WriteLine("Please choose input method:");
                Console.WriteLine("1. Type command");
                Console.WriteLine("2. Voice command");
                Console.Write("Enter choice (1 or 2): ");
                var inputChoice = Console.ReadLine();

                string commandInput;

                switch (inputChoice)
                {
                    case "1":
                        Console.Write("Enter text command: ");
                        commandInput = Console.ReadLine() ?? string.Empty;
                        break;
                    case "2":
                        Console.WriteLine("Enter voice command:");
                        commandInput = await voiceInputHandler.GetVoiceCommandAsync();
                        Console.WriteLine($"You said: {commandInput}");
                        break;
                    default:
                        Console.WriteLine("Invalid choice. Please try again.");
                        continue;
                }

                var result = await commandInvoker.ExecuteCommandAsync(commandInput);

                Console.WriteLine(result);
                await textToSpeechService.SpeakAsync(result);

                if (commandInput.Trim().Equals("exit", StringComparison.OrdinalIgnoreCase))
                {
                    isRunning = false;
                }
            }
        }
    }
}
```

**Explanation:**
  - **User Choice for Input Method:** The user can choose between typing a command or using voice input.
  - **Incorporated `VoiceInputHandler`:** Used to capture voice commands.
  - **Updated Main Loop:** Adjusted to handle both input methods.
  - **Asynchronous Methods:** Ensured that all asynchronous methods are awaited properly.

---

## 4.4 Testing the Application

### 4.4.1 Running the Application

```bash
dotnet run
```

### 4.4.2 Testing Voice Input

1. **Choose Voice Input:**
	- When prompted, enter `2` to select voice command input.

2. **Record a Voice Command:**
	- Press Enter to start recording.
	- Speak a command clearly into the microphone (e.g., "navigate to example.com").
	- Press Enter to stop recording.

4. **Transcription:**
	- The application will transcribe your voice input and display the recognized command.

5. **Command Execution:**
	- The application will execute the command as if you had typed it.
	- Observe the results and ensure they match your expectations.

6. **Voice Output:**
	- The application's response will be displayed and read aloud.

### 4.4.3 Testing with Various Commands

**Examples:**
  - "navigate to example.com"
  - "summarize the page"
  - "question what is this page about?"

**Verify:**
  - The speech-to-text service accurately transcribes your commands.
  - Commands are executed correctly.
  - Responses are displayed and read aloud.

### 4.4.4 Handling Errors

**If Transcription Fails:**
  - Ensure your microphone is working properly.
  - Speak clearly and minimize background noise.
  - Check for any error messages in the console.

**If Commands Are Not Recognized:**
  - Verify that the transcribed text matches your intended command.
  - Ensure that the commands follow the expected format.

---

## 4.5 Error Handling and Logging

Ensure that exceptions are properly handled and informative messages are provided to the user.

### 4.5.1 Enhancing `SpeechToTextService` Error Handling

- **Already Implemented:** The `TranscribeAudioAsync` method logs errors if the transcription fails.

### 4.5.2 Handling Cancellation

- **Check for Cancellation:** Ensure that when the recording is canceled, the application handles it gracefully.

---

## 4.6 Recap and Next Steps

### 4.6.1 What We've Achieved

**Implemented Speech-to-Text Functionality:**
  - Created `ISpeechToTextService` and its implementation `SpeechToTextService`.
  - Utilized OpenAI's Whisper API to transcribe audio input into text commands.

**Implemented Voice Input Handler:**
  - Implemented keyboard start & stop voice recording interaction.
  - Used `CancellationToken` to control the recording process.

**Updated Main Program Loop:**
  - Allowed users to choose between typing commands and using voice input.
  - Incorporated the speech-to-text service into the application flow.

**Maintained Clean Code Architecture:**
  - Organized code into appropriate namespaces.
  - Used Dependency Injection to manage services and promote loose coupling.

**Enhanced Accessibility:**
  - Improved the user experience for visually impaired users by allowing voice commands.

### 4.6.2 Final Thoughts

Throughout this workshop, we've built an accessible web navigation tool that leverages AI and modern .NET technologies to assist users. By adding speech-to-text capabilities, we've made our application even more accessible and user-friendly.

---

### 4.6.3 Additional Resources

- **OpenAI Whisper API Documentation:** [OpenAI Whisper API](https://platform.openai.com/docs/guides/speech-to-text)
- **NAudio Documentation:** [NAudio GitHub Repository](https://github.com/naudio/NAudio)
- **Dependency Injection in .NET:** [Microsoft Docs - Dependency Injection](https://learn.microsoft.com/en-us/dotnet/core/extensions/dependency-injection)
- **Asynchronous Programming in C#:** [Microsoft Docs - Async/Await](https://learn.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/async/)

---

### 4.6.4 Q&A Session

Feel free to ask any questions or share any challenges you encountered during this block.

---

### 4.6.5 Next Steps

In Block 4, we'll empower users to interact with web pages using voice commands, bringing us closer to a fully interactive, accessible web navigation tool.

Thank you for your participation so far! Let's take a short break before we move on to Block 5.

---

# Workshop Block 5: Enhancing Interaction with AI-Powered Commands

**Duration:** 50 min. + 10 min. break

Welcome back! In this block, we'll further enhance our application by implementing two new AI-powered commands that allow users to interact more effectively with web pages:

1. **List Available Actions:** A command that generates a list of possible actions/interactions based on the current page's HTML content.
2. **Perform Specific Action:** A command that allows the user to request a specific action, and the application will identify and interact with the corresponding HTML element using Playwright.

We'll leverage OpenAI's capabilities to analyze the HTML content and assist in identifying actionable elements.

---

## **Objectives**

- **Implement a command to list possible actions/interactions on the current page.**
- **Implement a command to perform a user-specified action on the page.**
- **Enhance the OpenAI service to analyze HTML content and generate actionable insights.**
- **Utilize Playwright to interact with HTML elements based on AI-generated selectors.**
- **Maintain clean code architecture by organizing code into appropriate namespaces and using Dependency Injection.**

---

## **Prerequisites**

Before we begin, ensure you have:

- **Completed Blocks 1 to 4**, with the application capable of navigating to web pages, summarizing content, reading aloud, and answering questions.
- **An active OpenAI API key** stored in your environment variables (`OPENAI_API_KEY`).
- **Internet connectivity** for accessing web pages and OpenAI's API.
- **Speakers or headphones** connected for audio output (optional but recommended).

---

## 5.1 Enhancing the OpenAI Service

We'll first update our `IOpenAIService` and `OpenAIService` to include methods that generate possible actions and identify specific elements based on user requests.

### 5.1.1 Updating the `IOpenAIService` Interface

Add new methods to handle action listing and element identification.

**Updated `IOpenAIService.cs`:**

```csharp
namespace AccessibleWebNavigator.Services;

public interface IOpenAIService
{
    Task<string> SummarizeTextAsync(string textContent);
    Task<string> AnswerQuestionAsync(string textContent, string question);
    Task<string[]> GetPossibleActionsAsync(string htmlContent);
    Task<string> GetElementSelectorAsync(string actionDescription, string htmlContent);
}
```

- **Explanation:**
  - **GetPossibleActionsAsync:** Generates a list of possible actions/interactions based on the HTML content.
  - **GetElementSelectorAsync:** Identifies the CSS selector of an HTML element based on the user's action description.

### 5.1.2 Implementing New Methods in `OpenAIService`

Update the `OpenAIService` class to implement these methods.

**Updated `OpenAIService.cs`:**

```csharp
using System.Text.RegularExpressions;
using OpenAI.Chat;

namespace AccessibleWebNavigator.Services;

public class OpenAIService : IOpenAIService
{
    private readonly ChatClient _chatClient;

    public OpenAIService(IApiKeyProvider apiKeyProvider)
    {
        var apiKey = apiKeyProvider.GetOpenAiApiKey();
        _chatClient = new ChatClient("gpt-3.5-turbo", apiKey);
        // If you have your own API Key, you can try different models as well, but be aware of the billing costs
        // here are some suggestions: gpt-4, gpt-4-turbo, gpt-4o-mini, o1-mini, o1-preview
        // for more details about models and their parameters, visit: https://platform.openai.com/docs/models
        // if you want to learn more about model pricing, visit: https://openai.com/api/pricing/
    }

    public async Task<string> SummarizeTextAsync(string textContent)
    {
        var messages = new List<ChatMessage>
        {
            new SystemChatMessage("You are a helpful assistant that summarizes web page content."),
            new UserChatMessage($"Please provide a concise summary of the following content:\n\n{textContent}")
        };
        
        return await GenerateAsync(messages);
    }

    public async Task<string> AnswerQuestionAsync(string textContent, string question)
    {
        var messages = new List<ChatMessage>
        {
            new SystemChatMessage("You are a helpful assistant that answers questions based on the provided web page content."),
            new UserChatMessage(
                $"Based on the following web page content, please answer the question:\n\n" + 
                $"Content:\n{textContent}\n\nQuestion:\n{question}")
        };

        return await GenerateAsync(messages);
    }

    public async Task<string[]> GetPossibleActionsAsync(string htmlContent)
    {
        var messages = new List<ChatMessage>
        {
            new SystemChatMessage(
                "You are a helpful assistant that is able to " + 
                "list available web page interactions based on " + 
                "the provided web page content " + 
                "for visually impaired users."),
            new UserChatMessage(
                "Based on the following HTML content, " + 
                "list the available user actions or interactions in a numbered list. " + 
                "Only list the actions without any additional explanation.\n" + 
                $"Here is the HTML content:\n\n===\n{htmlContent}\n===\n")
        };

		var actionsText = await GenerateAsync(messages);;
        var actions = actionsText.Split('\n').Select(a => Regex.Replace(a, @"^\d+\.\s*", "")).ToArray();
        return actions;
    }

    public async Task<string> GetElementSelectorAsync(string actionDescription, string htmlContent)
    {
        var messages = new List<ChatMessage>
        {
            new SystemChatMessage(
                "You are a helpful assistant that is able to " + 
                "provide the CSS selector based on user's description and the HTML content. " + 
                "The CSS selector should match exactly the element that is described by the user's action description. " + 
				"Only provide the CSS selector and nothing else. "),
            new UserChatMessage(
                "Based on the following HTML content, " + 
                "list the available user actions or interactions in a numbered list. " + 
                "Only list the actions without any additional explanation.\n" + 
                $"Here is the user's action description:\n\n{actionDescription}\n\n" +
                $"Here is the HTML content:\n\n===\n{htmlContent}\n===\n" + 
                "CSS selector:\n")
        };

        var result = await GenerateAsync(messages);
        result = result.Trim('`');
        return result;
    }

    private async Task<string> GenerateAsync(List<ChatMessage> messages)
    {
        try
        {
            var response = await _chatClient.CompleteChatAsync(messages);
            return response.Value.ToString();
        }
        catch (Exception ex)
        {
            Console.Error.WriteLine($"Error generating response: {ex.Message}");
            return string.Empty;
        }
    }
}
```

- **Explanation:**
  - **GetPossibleActionsAsync:** Uses OpenAI to analyze the HTML content and generate a list of possible actions.
  - **GetElementSelectorAsync:** Uses OpenAI to generate a CSS selector for the element that matches the user's action description.

## 5.2 Extending the WebNavigator to Retrieve HTML Content

### 5.2.1 Updating the `IWebNavigator` Interface

We'll add a new method `GetPageHtmlContentAsync` to the `IWebNavigator` interface.

```csharp
namespace AccessibleWebNavigator.Services;

public interface IWebNavigator
{
    Task<bool> NavigateToAsync(string url);
    Task<string> GetInnerTextContentAsync();
    Task<string> GetPageHtmlContentAsync();
}
```

**Explanation:**

- **GetPageHtmlContentAsync:** Asynchronously retrieves the HTML content of the current page.

### 5.2.2 Implementing the Method in `WebNavigator`

We'll implement the new method in the `WebNavigator` class.

**Updated `WebNavigator.cs`:**

```csharp
using Microsoft.Playwright;
using System.Net;

namespace AccessibleWebNavigator.Services;

public class WebNavigator : IWebNavigator
{
    private readonly IPage _page;

    public WebNavigator(IPage page)
    {
        _page = page ?? throw new ArgumentNullException(nameof(page));
    }

    public async Task<bool> NavigateToAsync(string url)
    {
        try
        {
            var response = await _page.GotoAsync(url);
            return response == null || response.Status == (int)HttpStatusCode.OK;
        }
        catch (Exception ex)
        {
            Console.Error.WriteLine($"Error navigating to {url}: {ex.Message}");
            return false;
        }
    }

    public async Task<string> GetInnerTextContentAsync()
    {
        try
        {
            var content = await _page.InnerTextAsync("body");
            return content;
        }
        catch (Exception ex)
        {
            Console.Error.WriteLine($"Error getting page content: {ex.Message}");
            return string.Empty;
        }
    }

    public async Task<string> GetPageHtmlContentAsync()
    {
        try
        {
            var htmlContent = await _page.ContentAsync();
            return htmlContent;
        }
        catch (Exception ex)
        {
            Console.Error.WriteLine($"Error getting page HTML content: {ex.Message}");
            return string.Empty;
        }
    }
}
```

**Explanation:**

- **GetPageHtmlContentAsync:** Uses Playwright's `ContentAsync` method to retrieve the entire HTML content of the current page.
- **Error Handling:** Returns an empty string if an exception occurs during retrieval.

---

## 5.3 Implementing the Command to List Possible Actions

We'll create a new command that, when invoked, provides the user with a list of possible actions based on the current page's HTML content.

### 5.3.1 Creating the `ListActionsCommand`

Create a new file in the **Commands** folder:

**Commands/ListActionsCommand.cs**

```csharp
using AccessibleWebNavigator.Services;

namespace AccessibleWebNavigator.Commands;

public class ListActionsCommand : ICommand
{
    private readonly IWebNavigator _webNavigator;
    private readonly IOpenAIService _openAIService;

    public ListActionsCommand(IWebNavigator webNavigator, IOpenAIService openAIService)
    {
        _webNavigator = webNavigator ?? throw new ArgumentNullException(nameof(webNavigator));
        _openAIService = openAIService ?? throw new ArgumentNullException(nameof(openAIService));
    }

    public bool CanExecute(string commandInput) => 
        commandInput.Trim().ToLower().StartsWith("list actions", StringComparison.OrdinalIgnoreCase) ||
        commandInput.Trim().ToLower().StartsWith("actions", StringComparison.OrdinalIgnoreCase);

    public async Task<string> ExecuteAsync(string commandInput)
    {
        var htmlContent = await _webNavigator.GetPageHtmlContentAsync();

        if (string.IsNullOrEmpty(htmlContent))
        {
            return "No page content available. Please navigate to a page first.";
        }

        var actions = await _openAIService.GetPossibleActionsAsync(htmlContent);

        if (actions.Length > 0)
        {
            return "Possible actions:\n" + string.Join("\n", actions);
        }
        else
        {
            return "No actions could be determined.";
        }
    }
}
```

- **Explanation:**
  - **CanExecute:** Checks if the user's input matches phrases that indicate they want to list possible actions.
  - **ExecuteAsync:** Retrieves the HTML content and uses the OpenAI service to generate the list of actions.

---

## 5.4 Implementing the Command to Perform a Specific Action

We'll create a command that allows the user to specify an action they want to perform, and the application will interact with the corresponding element on the page.

### 5.4.1 Creating the `PerformActionCommand`

Create a new file in the **Commands** folder:

**Commands/PerformActionCommand.cs**

```csharp
using AccessibleWebNavigator.Services;
using Microsoft.Playwright;

namespace AccessibleWebNavigator.Commands;

public class PerformActionCommand : ICommand
{
    private readonly IWebNavigator _webNavigator;
    private readonly IOpenAIService _openAIService;
    private readonly IPage _page;

    public PerformActionCommand(IWebNavigator webNavigator, IOpenAIService openAIService, IPage page)
    {
        _webNavigator = webNavigator ?? throw new ArgumentNullException(nameof(webNavigator));
        _openAIService = openAIService ?? throw new ArgumentNullException(nameof(openAIService));
        _page = page ?? throw new ArgumentNullException(nameof(page));
    }

    public bool CanExecute(string commandInput)
    {
        string lowerInput = commandInput.Trim().ToLower();
        return lowerInput.StartsWith("click ") || lowerInput.StartsWith("select ");
    }

    public async Task<string> ExecuteAsync(string commandInput)
    {
        var actionDescription = commandInput.Trim();

        var htmlContent = await _webNavigator.GetPageHtmlContentAsync();

        if (string.IsNullOrEmpty(htmlContent))
        {
            return "No page content available. Please navigate to a page first.";
        }

        var selector = await _openAIService.GetElementSelectorAsync(htmlContent, actionDescription);

        if (string.IsNullOrEmpty(selector))
        {
            return "Could not find the element to perform the action.";
        }

        try
        {
            await _page.ClickAsync(selector, new PageClickOptions{ Timeout = 5000 });
            return $"Action '{actionDescription}' performed successfully.";
        }
        catch (Exception ex)
        {
            return $"Failed to perform the action: {ex.Message}";
        }
    }
}
```

- **Explanation:**
  - **CanExecute:** Checks if the command starts with "click " or "select ".
  - **ExecuteAsync:** Retrieves the page's HTML content, uses the OpenAI service to get the CSS selector for the element, and then uses Playwright to interact with the element.

---

## 5.5 Updating the Service Configuration

Add the new commands to the service configuration.

### 5.5.1 Updating `ServiceConfigurator.cs`

**Updated `ServiceConfigurator.cs`:**

```csharp
using Microsoft.Extensions.DependencyInjection;
using AccessibleWebNavigator.Commands;
using Microsoft.Playwright;

namespace AccessibleWebNavigator.Services
{
    public static class ServiceConfigurator
    {
        public static async Task<ServiceProvider> ConfigureAsync()
        {
            var services = new ServiceCollection();

            // Register services
            services.AddSingleton<IApiKeyProvider, EnvironmentApiKeyProvider>();
            services.AddSingleton<IOpenAIService, OpenAIService>();
            services.AddSingleton<ITextToSpeechService, TextToSpeechService>();
            services.AddSingleton<ISpeechToTextService, SpeechToTextService>();

            // Initialize Playwright, Browser, and Page
            var playwright = await Playwright.CreateAsync();
            var browser = await playwright.Chromium.LaunchAsync(new BrowserTypeLaunchOptions { Headless = false });
            var page = await browser.NewPageAsync();

            // Register IPage as a singleton
            services.AddSingleton(page);

            // Register IWebNavigator, which depends on IPage
            services.AddSingleton<IWebNavigator, WebNavigator>();

            // Register commands
            services.AddSingleton<ICommand, ExitCommand>();
            services.AddSingleton<ICommand, NavigateCommand>();
            services.AddSingleton<ICommand, SummarizeCommand>();
            services.AddSingleton<ICommand, QuestionAnswerCommand>();
            services.AddSingleton<ICommand, ListActionsCommand>();
            services.AddSingleton<ICommand, PerformActionCommand>();

            // Register utilities
            services.AddSingleton<CommandInvoker>();
            services.AddSingleton<VoiceInputHandler>();

            return services.BuildServiceProvider();
        }
    }
}
```

- **Explanation:**
  - **Registered New Commands:** Added `ListActionsCommand` and `PerformActionCommand` to the services.
  - **Dependencies:** Ensure that all required services are registered so that dependencies can be injected.

---

## 5.6 Testing the Application

### 5.6.1 Running the Application

```bash
dotnet run
```

### 5.6.2 Testing the List Actions Command

1. **Navigate to a Web Page:**

   - **Input:** 

```
navigate to example.com
```

   - **Expected Behavior:** The browser navigates to `https://example.com`.

2. **List Possible Actions:**

   - **Input:** 

```
list actions
```

   - **Expected Output:**
     - A list of possible actions based on the page content.
     - Example:

```
Possible actions:
Click on the link "More information..." to navigate to https://www.iana.org/domains/example.
```

### 5.6.3 Testing the Perform Action Command

- **Perform an Action:**

   - **Input:** 

```
click more information
```

   - **Expected Behavior:**
     - The application identifies the element associated with "More information" and clicks it.
     - The browser navigates or updates accordingly.
     - Output: `Action 'click more information' performed successfully.`

### 5.6.4 Observing Browser Interaction

- **Verify that Playwright performs the actions in the browser.**
- **Ensure that interactions are visible and correspond to the commands issued.**

---

## 5.7 Recap and Conclusion

### 5.7.1 What We've Achieved

In this final block, we've significantly enhanced our application's capabilities:

- **Implemented AI-Powered Interaction Commands:**
    - Created `ListActionsCommand` to provide users with possible actions based on page content.
    - Created `PerformActionCommand` to perform user-specified actions on the page.
- **Enhanced OpenAI Service:**
    - Added methods to analyze HTML content and generate actionable insights.
    - Used OpenAI to identify elements and generate CSS selectors.
- **Extended WebNavigator Functionality:**
    - Added `GetPageHtmlContentAsync` to retrieve the raw HTML content of the current page.
    - Ensured clear separation between retrieving text content and HTML content.
- **Integrated Playwright for Interaction:**
    - Utilized Playwright to interact with HTML elements based on AI-generated selectors.
- **Maintained Clean Code Architecture:**
    - Continued using meaningful namespaces and organized code for clarity.
    - Leveraged Dependency Injection to manage services and promote loose coupling.
- **Tested the Application:**
    - Verified that the new commands work as expected.
    - Ensured that browser interactions correspond to user commands.

### 5.7.2 Potential Next Steps

After the workshop, you can continue to focus on:

- **Improving the Accuracy of Element Selection:**
	- Explore ways to enhance the AI's ability to generate accurate selectors.
	- Implement fallback mechanisms if the AI-generated selector fails.
- **Enhancing Error Handling and User Feedback:**
	- Provide more informative messages and suggestions when actions fail.
	- Implement logging for debugging and monitoring.
- **Using Realtime API for Speech-to-Speech Communication**:
	- [Introducing the Realtime API | OpenAI](https://openai.com/index/introducing-the-realtime-api/)

Want an additional workshop Block for these? Let me know at peter.milovcik@siemens-healthineers.com

### 5.7.3 Final Thoughts

Throughout this workshop, we've built an accessible web navigation tool that leverages AI and modern .NET technologies to assist visually impaired users and enhance web interaction. We've:

- **Integrated OpenAI's Language Model:** For summarization, question answering, and generating actionable insights.
- **Added Text-to-Speech Capabilities:** To read out summaries and responses, improving accessibility.
- **Added Speech-to-Text Capabilities:** To enable voice commands, improving accessibility.
- **Utilized Playwright:** For web navigation and interaction, automating browser tasks.

We've maintained a clean and scalable code architecture, making use of the Command design pattern and Dependency Injection to ensure our application is modular and easy to maintain.

---

### 5.7.4 Additional Resources

- **OpenAI API Documentation:** [OpenAI API Reference](https://platform.openai.com/docs/api-reference/introduction)
- **Playwright for .NET Documentation:** [Playwright .NET](https://playwright.dev/dotnet/docs/intro)
- **Dependency Injection in .NET:** [Microsoft Docs - Dependency Injection](https://learn.microsoft.com/en-us/dotnet/core/extensions/dependency-injection)
- **Exception Handling Best Practices:** [Microsoft Docs - Exception Handling](https://learn.microsoft.com/en-us/dotnet/standard/exceptions/best-practices-for-exceptions)

---

### 5.7.5 Q&A Session

Feel free to ask any questions or share any challenges you encountered during this block.

---

## 5.8 Closing Remarks

Thank you for your dedication and hard work throughout this workshop. We've covered a lot of ground and built a powerful, accessible web navigation tool that demonstrates the potential of combining AI with modern development practices.

By leveraging AI services and automating web interactions, we've created an application that not only enhances accessibility but also opens up new possibilities for user interaction with web content.

We encourage you to continue exploring and enhancing the application!

---

## 5.9 Acknowledgments

Thank you for your participation and commitment. We hope this workshop has been informative and inspiring, and that you'll continue to apply these skills in your future projects.

---

## 5.10 Staying in Touch

Thank you for your participation in this workshop. I would be delighted to stay connected and continue our professional journey together. Please feel free to connect with me on LinkedIn:

https://www.linkedin.com/in/peter-milovcik/

Let's keep the conversation going! I'm looking forward to seeing how you apply what we've covered and the amazing projects you'll develop in the future.

---

**End of Workshop**

---

> *Happy Coding!*

---
